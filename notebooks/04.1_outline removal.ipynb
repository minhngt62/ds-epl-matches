{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7533263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import joblib\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "import heapq\n",
    "\n",
    "\n",
    "PRJ_ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prepare data \n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, RepeatedStratifiedKFold\n",
    "from sklearn.cluster import OPTICS, DBSCAN, Birch\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, IsolationForest\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# evaluate\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "lda_table = pd.DataFrame(columns = ['name', 'train_acc', 'train_f1', 'train_roc_auc','test_acc', 'test_f1','test_roc_auc'])\n",
    "pca_table = pd.DataFrame(columns = ['name', 'train_acc', 'train_f1', 'train_roc_auc','test_acc', 'test_f1','test_roc_auc'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8b361f",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratify(df, frac, random_state):\n",
    "    win_19_20 = df[(df['season'] == '2019/20') & (df['result'] == 'win')].sample(frac = frac, random_state = random_state)\n",
    "    lose_19_20 = df[(df['season'] == '2019/20') & (df['result'] == 'lose')].sample(frac = frac, random_state = random_state)\n",
    "    draw_19_20 = df[(df['season'] == '2019/20') & (df['result'] == 'draw')].sample(frac = frac, random_state = random_state)\n",
    "    win_20_21 = df[(df['season'] == '2020/21') & (df['result'] == 'win')].sample(frac = frac, random_state = random_state)\n",
    "    lose_20_21 = df[(df['season'] == '2020/21') & (df['result'] == 'lose')].sample(frac = frac, random_state = random_state)\n",
    "    draw_20_21 = df[(df['season'] == '2020/21') & (df['result'] == 'draw')].sample(frac = frac, random_state = random_state)\n",
    "    win_21_22 = df[(df['season'] == '2021/22') & (df['result'] == 'win')].sample(frac = frac, random_state = random_state)\n",
    "    lose_21_22 = df[(df['season'] == '2021/22') & (df['result'] == 'lose')].sample(frac = frac, random_state = random_state)\n",
    "    draw_21_22 = df[(df['season'] == '2021/22') & (df['result'] == 'draw')].sample(frac = frac, random_state = random_state)\n",
    "    df_test = pd.concat([win_19_20,lose_19_20,draw_19_20,win_20_21,lose_20_21,draw_20_21,win_21_22,lose_21_22,draw_21_22], axis = 0)\n",
    "    df_train = pd.concat([df,df_test]).drop_duplicates(keep=False)\n",
    "    return df_train.iloc[:,:-2], df_test.iloc[:,:-2],df_train.iloc[:,-2] , df_test.iloc[:,-2]\n",
    "\n",
    "def train_classifier(clf, X, y):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X, y)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "    \n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    \n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
    "    y_pred = cross_val_predict(clf, features, target.squeeze(),cv = 3, method = 'predict_proba')\n",
    "    y_pred_1 = cross_val_predict(clf, features, target.squeeze(),cv = 3, method = 'predict')\n",
    "    return f1_score(target.squeeze(), y_pred_1, average='weighted'), accuracy_score(target.squeeze(), y_pred_1), roc_auc_score(target.squeeze(), y_pred, multi_class = 'ovo', average = 'macro')\n",
    "\n",
    "\n",
    "    \n",
    "def train_predict(name, clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    global lda_table, pca_table\n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    f1tr, acctr,roctr = predict_labels(clf, X_train, y_train)\n",
    "    print(\"F1 score and accuracy score and roc score for training set: {:.4f} , {:.4f} , {:.4f}.\".format(f1tr , acctr, roctr))\n",
    "    \n",
    "    f1te, accte, rocte = predict_labels(clf, X_test, y_test)\n",
    "    print(\"F1 score and accuracy score and roc score for test set: {:.4f} , {:.4f} , {:.4f}.\".format(f1te , accte, rocte))\n",
    "    if 'lda' in name:\n",
    "        lda_table = pd.concat([lda_table,pd.DataFrame([[name, acctr,f1tr,roctr,accte,f1te,rocte]], columns = lda_table.columns)])\n",
    "    else:\n",
    "        pca_table = pd.concat([pca_table,pd.DataFrame([[name, acctr,f1tr,roctr,accte,f1te,rocte]], columns = lda_table.columns)])\n",
    "\n",
    "def tuning(clf,param_dict,X_train, y_train, n_iter=50,cv=5,scoring=\"roc_auc_ovo\",random_state=42,verbose=0):\n",
    "    search_spaces = {}\n",
    "    for param in clf.get_params().keys():\n",
    "        if (str(clf.__class__.__name__) +'_'+param) in param_dict.keys():\n",
    "            search_spaces[param] = param_dict[str(clf.__class__.__name__) +'_'+param]\n",
    "    if search_spaces == {}:\n",
    "        return clf\n",
    "    search = BayesSearchCV(clf, search_spaces, \n",
    "                           n_iter=n_iter, cv=cv, scoring=scoring, \n",
    "                           random_state=random_state, n_jobs=-1, verbose=verbose)\n",
    "\n",
    "    search.fit(X_train, y_train.squeeze())\n",
    "    \n",
    "    print(str(clf.__class__.__name__) +\" best score :\", search.best_score_)\n",
    "    print(str(clf.__class__.__name__) +\" best params:\", search.best_params_)\n",
    "\n",
    "    return search\n",
    "\n",
    "# Setup to save/load the model\n",
    "def save_model(model, id_):\n",
    "    print(\"Saving model\", id_)\n",
    "    joblib.dump(model, os.path.join(NOTE_ROOT_DIR, \"models\", id_ + \".pkl\"))\n",
    "def load_model(id_):\n",
    "    print(\"Loading model\", id_)\n",
    "    return joblib.load(os.path.join(NOTE_ROOT_DIR, \"models\", id_ + \".pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69eac0",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088bcef6",
   "metadata": {},
   "source": [
    "### LDA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(PRJ_ROOT_DIR, \"data\", \"tabular\", \"integrate\",\"lda\", \"matches.csv\"))\n",
    "para = {'name':'lda', 'eps': 8.5}\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab008fa",
   "metadata": {},
   "source": [
    "### PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7317590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(PRJ_ROOT_DIR, \"data\", \"tabular\", \"integrate\",\"pca\", \"matches.csv\"))\n",
    "para = {'name':'pca', 'eps': 30}\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffceb2",
   "metadata": {},
   "source": [
    "### 1.1. Manage Empty Positions' Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c86dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df[i].replace({-100 : min(df[df[i] != -100][i])}, inplace = True)\n",
    "#     df[i].replace({-100 : 0}, inplace = True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0aafc",
   "metadata": {},
   "source": [
    "## 2. Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scale =df[df.columns[2:]]\n",
    "scaler = StandardScaler()\n",
    "df_scale=scaler.fit_transform(df[df.columns[2:]])\n",
    "df_scale = pd.DataFrame(df_scale, columns = df.columns[2:])\n",
    "df_scale[['result','season'] ] = df[['home_result', 'season']]\n",
    "df_scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96a272f",
   "metadata": {},
   "source": [
    "## 2.Trainset & Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afd7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_scale\n",
    "# X_train, X_test, y_train, y_test = stratify(df, 0.2,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47145414",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_scale \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-2]], df['result'], test_size = 0.2, random_state = 42)\n",
    "y_train = y_train.replace({'lose':0, 'draw': 1, 'win': 2})\n",
    "y_test = y_test.replace({'lose':0, 'draw': 1, 'win': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train size :',len(X_train))\n",
    "print('Test size: ', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ca1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e211b5",
   "metadata": {},
   "source": [
    "## 3. Outline removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65558e5a",
   "metadata": {},
   "source": [
    "### 3.2. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb6738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.1),max_features=1.0)\n",
    "X_train['anomaly'] = model.fit_predict(X_train)\n",
    "ano = X_train[X_train['anomaly'] == -1].index\n",
    "X_train.drop(ano, inplace=True)\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "y_train.drop(ano, inplace=True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "assert len(X_train) == len(y_train)\n",
    "\n",
    "X_train = X_train[X_train.columns[:-1]]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d4df2",
   "metadata": {},
   "source": [
    "### 3.3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['cluster'] = DBSCAN(min_samples=2, eps = para['eps']).fit_predict(X_train)\n",
    "cls = X_train[X_train['cluster'] == -1].index\n",
    "X_train.drop(cls, inplace=True)\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "y_train.drop(cls, inplace=True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "\n",
    "X_train = X_train[X_train.columns[:-1]]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf8ef45",
   "metadata": {},
   "source": [
    "### 3.4. Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f5e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = X_train.columns\n",
    "upper = []\n",
    "lower = []\n",
    "for i in columns :\n",
    "    \n",
    "    Q1 = np.percentile(X_train[i], 25,\n",
    "                       interpolation = 'midpoint')\n",
    "\n",
    "    Q3 = np.percentile(X_train[i], 75,\n",
    "                       interpolation = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Upper bound\n",
    "    upper += np.where(X_train[i] >= (Q3+1.5*IQR))[0].tolist()\n",
    "#     print(upper)\n",
    "    # Lower bound\n",
    "    lower += np.where(X_train[i] <= (Q1-1.5*IQR))[0].tolist()\n",
    "\n",
    "    ''' Removing the Outliers '''\n",
    "index = upper+ lower\n",
    "frq = [index.count(i) for i in range(len(X_train))]\n",
    "# print([frq[i] for i in range(len(frq)) if frq[i] in heapq.nlargest(4, set(frq)) ])\n",
    "bl = [i for i in range(len(frq)) if frq[i] in heapq.nlargest(2, set(frq)) ]\n",
    "X_train.drop(bl, inplace = True)\n",
    "X_train.reset_index(inplace = True, drop = True)\n",
    "y_train.drop(bl, inplace = True)\n",
    "y_train.reset_index(inplace = True, drop = True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5070e9a4",
   "metadata": {},
   "source": [
    "## 6. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be565223",
   "metadata": {},
   "source": [
    "### 6.2. Attribute selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ff64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = np.zeros((1,len(X_train.columns)))\n",
    "#     model = XGBClassifier()\n",
    "model = RandomForestClassifier()\n",
    "param_dict = {  \n",
    "    \"XGBClassifier_eta\": (0.01,0.2,\"uniform\"),\n",
    "    \"XGBClassifier_min_child_weight\": (1,20),\n",
    "    \"XGBClassifier_max_depth\": (3,10),\n",
    "    \n",
    "    \"RandomForestClassifier_n_estimators\": (5, 500), \n",
    "    \"RandomForestClassifier_criterion\": [\"gini\", \"entropy\"],\n",
    "    \"RandomForestClassifier_max_depth\": (1, 19), # 19 overfits the data\n",
    "    \"RandomForestClassifier_min_samples_split\": (2, 20),\n",
    "    \"RandomForestClassifier_max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"RandomForestClassifier_max_leaf_nodes\": (2, 159),\n",
    "    \"RandomForestClassifier_min_impurity_decrease\": (1e-6, 0.5, \"uniform\"),\n",
    "    \"RandomForestClassifier_max_samples\": (0.5, 1.0, \"uniform\")\n",
    "}\n",
    "model_t = tuning(model,param_dict,X_train, y_train)\n",
    "# fit the model\n",
    "model_t.best_estimator_.fit(X_train, y_train)\n",
    "# get importance\n",
    "importance = model_t.best_estimator_.feature_importances_\n",
    "\n",
    "# plot feature importance\n",
    "plt.figure(figsize=(30,40))\n",
    "plt.bar([x for x in range(len(importance))], [x for x in importance])\n",
    "plt.show()\n",
    "\n",
    "thes = float(input('Thres: '))\n",
    "# summarize feature importance\n",
    "idx = []\n",
    "for i,v in enumerate(importance):\n",
    "    if v > thes :\n",
    "        idx.append(i)\n",
    "        print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "print(len(idx),idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_target = X_train.corrwith(y_train)\n",
    "corr_other = X_train.corr()\n",
    "corr_table = corr_target.subtract(corr_other.mean(axis = 1)) \n",
    "attribute = corr_table.nlargest(60)\n",
    "idx = [list(X_train.columns).index(i) for i in attribute.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[X_train.columns[idx]]\n",
    "X_test = X_test[X_test.columns[idx]]\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6ded68",
   "metadata": {},
   "source": [
    "### 6.3.Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d706a8",
   "metadata": {},
   "source": [
    "#### 6.3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ad6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 42,multi_class=\"multinomial\")\n",
    "train_predict(para['name']+'_lr_no_tune',lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d070e58",
   "metadata": {},
   "source": [
    "#### 6.3.4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6872cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "train_predict(para['name']+'_dt_no_tune',dt, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea44711",
   "metadata": {},
   "source": [
    "#### 6.3.5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33796e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "train_predict(para['name']+'_rf_no_tune',rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c5726",
   "metadata": {},
   "source": [
    "#### 6.3.6. Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier(learning_rate=0.7, n_estimators=100)\n",
    "train_predict(para['name']+'_ab_no_tune',ab, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d467d2d",
   "metadata": {},
   "source": [
    "#### 6.3.7. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d422e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate=0.7, random_state=42)\n",
    "train_predict(para['name']+'_gb_no_tune',gb, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c369b2a",
   "metadata": {},
   "source": [
    "#### 6.3.8. Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True, random_state=42)\n",
    "train_predict(para['name']+'_svc_no_tune',svc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf0bc3",
   "metadata": {},
   "source": [
    "#### 6.3.9. Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(random_state=42)\n",
    "train_predict(para['name']+'_nn_no_tune',nn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66e29f5",
   "metadata": {},
   "source": [
    "### 6.4. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a525669",
   "metadata": {},
   "source": [
    "##### 6.4.0. Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {  \n",
    "    \"RandomForestClassifier_n_estimators\": (5, 500), \n",
    "    \"RandomForestClassifier_criterion\": [\"gini\", \"entropy\"],\n",
    "    \"RandomForestClassifier_max_depth\": (1, 19), # 19 overfits the data\n",
    "    \"RandomForestClassifier_min_samples_split\": (2, 20),\n",
    "    \"RandomForestClassifier_max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"RandomForestClassifier_max_leaf_nodes\": (2, 159),\n",
    "    \"RandomForestClassifier_min_impurity_decrease\": (1e-6, 0.5, \"uniform\"),\n",
    "    \"RandomForestClassifier_max_samples\": (0.5, 1.0, \"uniform\"),\n",
    "        \n",
    "    \"GradientBoostingClassifier_n_estimators\": (2, 100),\n",
    "    \"GradientBoostingClassifier_learning_rate\": Real(low=0.001, high=3, prior=\"uniform\"),\n",
    "    \"GradientBoostingClassifier_subsample\": Real(low=0.05, high=1.0, prior=\"uniform\"),\n",
    "    \"GradientBoostingClassifier_criterion\": [\"friedman_mse\", \"squared_error\"],\n",
    "    \"GradientBoostingClassifier_min_samples_split\": Real(low=1e-6, high=1.0, prior=\"uniform\"),\n",
    "    \"GradientBoostingClassifier_max_depth\": (1, 10),\n",
    "    \"GradientBoostingClassifier_min_impurity_decrease\": Real(low=1e-6, high=0.5, prior=\"uniform\"),\n",
    "    \"GradientBoostingClassifier_max_features\": [None, \"sqrt\", \"log2\"],\n",
    "    \"GradientBoostingClassifier_max_leaf_nodes\": (2, 100),\n",
    "                  \n",
    "    \"AdaBoostClassifier_n_estimators\": (2, 500),\n",
    "    \"AdaBoostClassifier_learning_rate\": Real(low=0.001, high=3,  prior='uniform'),\n",
    "                  \n",
    "    \"SVC_C\": Real(low=1e-6, high=2, prior=\"uniform\"),\n",
    "    \"SVC_kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"SVC_degree\": (2, 30),\n",
    "    \"SVC_gamma\": [\"scale\", \"auto\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915813c",
   "metadata": {},
   "source": [
    "#### 6.4.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f5e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"lr_t\")\n",
    "else:\n",
    "    lr_t = tuning(lr,param_dict,X_train, y_train)\n",
    "    save_model(lr_t,\"lr_t\")\n",
    "train_predict(para['name']+'_lr_tune',lr_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df6aed",
   "metadata": {},
   "source": [
    "#### 6.4.4. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb6892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"dt_t\")\n",
    "else:\n",
    "    dt_t = tuning(dt,param_dict,X_train, y_train)\n",
    "    save_model(dt_t,\"dt_t\")\n",
    "train_predict(para['name']+'_dt_tune',dt_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b2d62",
   "metadata": {},
   "source": [
    "#### 6.4.5. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc33e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"rf_t\")\n",
    "else:\n",
    "    rf_t = tuning(rf,param_dict,X_train, y_train).best_estimator_\n",
    "    save_model(rf_t,\"rf_t\")\n",
    "train_predict(para['name']+'_rf_tune',rf_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad383d8e",
   "metadata": {},
   "source": [
    "#### 6.4.6. Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"ab_t\")\n",
    "else:\n",
    "    ab_t = tuning(ab,param_dict,X_train, y_train).best_estimator_\n",
    "    save_model(ab_t,\"ab_t\")\n",
    "train_predict(para['name']+'_ab_tune',ab_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980dbc0f",
   "metadata": {},
   "source": [
    "#### 6.4.7. Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdc6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"gb_t\")\n",
    "else:\n",
    "    gb_t = tuning(gb,param_dict,X_train, y_train).best_estimator_\n",
    "    save_model(gb_t,\"gb_t\")\n",
    "train_predict(para['name']+'_gb_tune',gb_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e02252e",
   "metadata": {},
   "source": [
    "#### 6.4.8. Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d552a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"svc_t\")\n",
    "else:\n",
    "    svc_t = tuning(svc,param_dict,X_train, y_train).best_estimator_\n",
    "    save_model(svc_t,\"svc_t\")\n",
    "train_predict(para['name']+'_svc_tune',svc_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282f5a29",
   "metadata": {},
   "source": [
    "#### 6.4.9. Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ddbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load = False\n",
    "if load =  True:\n",
    "    load_model(\"nn_t\")\n",
    "else:\n",
    "    nn_t = tuning(nn,param_dict,X_train, y_train)\n",
    "    save_model(nn_t,\"nn_t\")\n",
    "train_predict(para['name']+'_nn_tune',nn_t, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411383f7",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bbad8a",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8880a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, rf.predict(X_test), target_names=[\"lose\",\"draw\",\"win\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "conf_mx = confusion_matrix(y, rf.predict(X))\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49698def",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578ad764",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2cd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, svc.predict(X_test), target_names=[\"draw\", \"lose\", \"win\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6437ad20",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test])\n",
    "y = pd.concat([y_train, y_test])\n",
    "conf_mx = confusion_matrix(y, svc.predict(X))\n",
    "row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "norm_conf_mx = conf_mx / row_sums\n",
    "\n",
    "np.fill_diagonal(norm_conf_mx, 0)\n",
    "plt.matshow(norm_conf_mx, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8aaf33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
